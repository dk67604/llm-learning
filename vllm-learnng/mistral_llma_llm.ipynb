{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5a397b",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to interact with a local vLLM OpenAI-compatible server for chat and completion tasks, including advanced features such as tool/function calling. It covers sending chat prompts, handling tool calls for weather information, and integrating external APIs (like wttr.in) to provide dynamic responses. The workflow showcases both direct HTTP requests and usage of the OpenAI Python client with custom endpoints.\n",
    "\n",
    "Additionally, the notebook includes examples using various models, such as Mistral-7B-Instruct, and Llama-3.1B-Instruct. For Llama-3.1B-Instruct, you can start the vLLM server with the appropriate model and chat template, then interact with it using the same OpenAI-compatible API for chat completions and tool calls. This allows you to leverage the capabilities of Llama-3.1B-Instruct for both standard and advanced conversational AI tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d8cdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 404 {\"object\":\"error\",\"message\":\"The model `TheBloke/Mistral-7B-Instruct-v0.2-AWQ` does not exist.\",\"type\":\"NotFoundError\",\"param\":null,\"code\":404}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "VLLM_URL = \"http://localhost:8000/v1/chat/completions\"\n",
    "API_KEY = \"token\"  # Use \"Bearer token\" in header — vLLM does not enforce auth by default\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Who is the best French painter? Answer in one short sentence.\"}\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(VLLM_URL, headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    reply = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(\"Assistant:\", reply)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516f86e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion results:\n",
      "Completion(id='cmpl-3ac37427f2d5489fa63f7a9218dafffa', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=Logprobs(text_offset=[0, 3, 4, 12, 15, 21, 22, 28, 30, 36, 42, 45, 50, 53, 58, 60], token_logprobs=[-0.3707573413848877, -0.07804243266582489, -0.0024917051196098328, -0.0005330810672603548, -0.0017513189231976867, -0.0010155049385502934, -0.00037079135654494166, -0.0016156489728018641, -7.807903602952138e-05, -0.0005930095794610679, -0.004116039723157883, -0.0003779412363655865, -5.0424259825376794e-05, -0.0006279165390878916, -4.012005805969238, -10.061521530151367], tokens=['▁or', ',', '▁through', '▁in', 'action', ',', '▁allow', '▁a', '▁human', '▁being', '▁to', '▁come', '▁to', '▁harm', '▁(', 'Zero'], top_logprobs=[{'▁or': -0.3707573413848877, ',': -1.3082573413848877, '.': -3.9645073413848877}, {',': -0.07804243266582489, '▁through': -2.640542507171631, '▁allow': -5.757730007171631}, {'▁through': -0.0024917051196098328, '<0x0A>': -7.354053974151611, '▁by': -7.635303974151611}, {'▁in': -0.0005330810672603548, '▁a': -8.867720603942871, '▁care': -9.859908103942871}, {'action': -0.0017513189231976867, 'activity': -6.90018892288208, '▁action': -8.150188446044922}, {',': -0.0010155049385502934, '▁or': -7.501015663146973, '▁allow': -8.055703163146973}, {'▁allow': -0.00037079135654494166, '▁allows': -9.000370979309082, 'allow': -9.250370979309082}, {'▁a': -0.0016156489728018641, '▁him': -6.548490524291992, '▁one': -10.814115524291992}, {'▁human': -7.807903602952138e-05, 'human': -10.773515701293945, '▁Human': -10.836015701293945}, {'▁being': -0.0005930095794610679, '▁to': -8.703718185424805, '<0x0A>': -8.844343185424805}, {'▁to': -0.004116039723157883, '▁come': -5.629116058349609, '<0x0A>': -8.34786605834961}, {'▁come': -0.0003779412363655865, '▁Come': -9.406627655029297, '▁be': -9.594127655029297}, {'▁to': -5.0424259825376794e-05, '▁about': -11.67192554473877, '<0x0A>': -11.69536304473877}, {'▁harm': -0.0006279165390878916, '▁harmful': -8.633440017700195, '▁hurt': -8.906877517700195}, {'▁(': -4.012005805969238, '.': -0.09794331341981888, ',': -3.5041933059692383}, {'Zero': -10.061521530151367, 'First': -0.9326151013374329, 'Is': -2.120115041732788, 'Pr': -2.424802541732788}]), text=' or, through inaction, allow a human being to come to harm (Zero', stop_reason=None, prompt_logprobs=None), CompletionChoice(finish_reason='length', index=1, logprobs=Logprobs(text_offset=[0, 1, 4, 5, 13, 16, 22, 23, 29, 31, 37, 43, 46, 51, 54, 59], token_logprobs=[-1.3082573413848877, -0.10698230564594269, -0.4637446999549866, -0.004421696066856384, -0.0009463122696615756, -0.002084703417494893, -0.005588503088802099, -0.000507464399561286, -0.0024723222013562918, -0.00014780859055463225, -0.0011474461061879992, -0.0036351096350699663, -0.0006385194719769061, -0.0001113352773245424, -0.0009117737063206732, -0.13002125918865204], tokens=[',', '▁or', ',', '▁through', '▁in', 'action', ',', '▁allow', '▁a', '▁human', '▁being', '▁to', '▁come', '▁to', '▁harm', '.'], top_logprobs=[{',': -1.3082573413848877, '▁or': -0.3707573413848877, '.': -3.9645073413848877}, {'▁or': -0.10698230564594269, '▁nor': -3.4663572311401367, '▁but': -4.122607231140137}, {',': -0.4637446999549866, '▁through': -1.0418696403503418, '▁by': -4.713744640350342}, {'▁through': -0.004421696066856384, '▁by': -6.613796710968018, '▁using': -6.637234210968018}, {'▁in': -0.0009463122696615756, '▁a': -8.399383544921875, '▁care': -8.961883544921875}, {'action': -0.002084703417494893, 'activity': -6.736459732055664, '▁action': -8.181772232055664}, {',': -0.005588503088802099, '▁or': -5.747776031494141, '▁allow': -6.208713531494141}, {'▁allow': -0.000507464399561286, '▁allows': -8.688007354736328, 'allow': -9.266132354736328}, {'▁a': -0.0024723222013562918, '▁him': -6.166534900665283, '▁human': -10.033721923828125}, {'▁human': -0.00014780859055463225, '▁Human': -9.976710319519043, 'human': -10.617335319519043}, {'▁being': -0.0011474461061879992, '▁to': -7.571459770202637, '<0x0A>': -8.407397270202637}, {'▁to': -0.0036351096350699663, '▁come': -5.816134929656982, '<0x0A>': -8.07394790649414}, {'▁come': -0.0006385194719769061, '▁be': -8.617826461791992, '▁Come': -9.024076461791992}, {'▁to': -0.0001113352773245424, '▁about': -10.75792407989502, '<0x0A>': -11.05479907989502}, {'▁harm': -0.0009117737063206732, '▁harmful': -8.438411712646484, '▁hurt': -8.485286712646484}, {'.': -0.13002125918865204, ',': -3.028458833694458, '▁(': -3.942521333694458}]), text=', or, through inaction, allow a human being to come to harm.', stop_reason=None, prompt_logprobs=None)], created=1749622142, model='solidrust/Mistral-7B-Instruct-v0.3-AWQ', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=32, prompt_tokens=10, total_tokens=42, completion_tokens_details=None, prompt_tokens_details=None), kv_transfer_params=None)\n"
     ]
    }
   ],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "# Completion API\n",
    "stream = False\n",
    "completion = client.completions.create(\n",
    "    model=model,\n",
    "    prompt=\"A robot may not injure a human being\",\n",
    "    echo=False,\n",
    "    n=2,\n",
    "    stream=stream,\n",
    "    logprobs=3)\n",
    "\n",
    "print(\"Completion results:\")\n",
    "if stream:\n",
    "    for c in completion:\n",
    "        print(c)\n",
    "else:\n",
    "    print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d90b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning_content for Round 1: None\n",
      "content for Round 1:  The number 9.11 is greater than the number 9.8. However, it's important to note that 9.11 does not represent a common mathematical expression or operation. Usually, when comparing numbers, we don't have a decimal point in the same place for both numbers. In this case, since 9.11 has a higher number before the decimal point, it is a greater number than 9.8.\n",
      "reasoning_content for Round 2: None\n",
      "content for Round 2:  There are no Rs in the word \"strawberry.\" The letter R appears neither in the word \"strawberry\" nor in its plural form \"strawberries.\"\n"
     ]
    }
   ],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "An example shows how to generate chat completions from reasoning models\n",
    "like DeepSeekR1.\n",
    "\n",
    "To run this example, you need to start the vLLM server with the reasoning \n",
    "parser:\n",
    "\n",
    "```bash\n",
    "vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "     --enable-reasoning --reasoning-parser deepseek_r1\n",
    "```\n",
    "\n",
    "This example demonstrates how to generate chat completions from reasoning models\n",
    "using the OpenAI Python client library.\n",
    "\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "# Round 1\n",
    "messages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater?\"}]\n",
    "response = client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(\"reasoning_content for Round 1:\", reasoning_content)\n",
    "print(\"content for Round 1:\", content)\n",
    "\n",
    "# Round 2\n",
    "messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"How many Rs are there in the word 'strawberry'?\",\n",
    "})\n",
    "response = client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(\"reasoning_content for Round 2:\", reasoning_content)\n",
    "print(\"content for Round 2:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7158571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion results:\n",
      "ChatCompletion(id='chatcmpl-37031a4e0a6e4d7bb8b0264b25e8752e', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-5f85a5be66904099b5b7ec2a7f074c6d', function=Function(arguments='{\"city\": \"San Jose\", \"state\": \"CA\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')], reasoning_content=None), stop_reason=128008)], created=1749624120, model='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=32, prompt_tokens=399, total_tokens=431, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None, kv_transfer_params=None)\n",
      "\n",
      "\n",
      "\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None)\n",
      "ChoiceDeltaToolCall(index=0, id='chatcmpl-tool-f5b595a2eae74a58be024b408172bd06', function=ChoiceDeltaToolCallFunction(arguments=None, name='get_current_weather'), type='function')\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"city\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='San', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' Jose\"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=', \"state\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='CA\"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=', \"unit\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='c', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='elsius\"}', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "streamed tool call id: chatcmpl-tool-f5b595a2eae74a58be024b408172bd06 \n",
      "streamed tool call name: get_current_weather\n",
      "streamed tool call arguments: {\"city\": \"San Jose\", \"state\": \"CA\", \"unit\": \"celsius\"}\n",
      "\n",
      "\n",
      "\n",
      "The weather in San Jose, CA is 13.0°C with partly cloudy.\n",
      "\n",
      "\n",
      "\n",
      "ChatCompletion(id='chatcmpl-53b01d31c87f4d13abf63594beab4e68', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The weather in San Jose, CA is 13.0°C with partly cloudy.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1749624124, model='hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=463, total_tokens=481, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None, kv_transfer_params=None)\n"
     ]
    }
   ],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "Set up this example by starting a vLLM OpenAI-compatible server with tool call\n",
    "options enabled. For example:\n",
    "\n",
    "IMPORTANT: for mistral, you must use one of the provided mistral tool call\n",
    "templates, or your own - the model default doesn't work for tool calls with vLLM\n",
    "See the vLLM docs on OpenAI server & tool calling for more details.\n",
    "\n",
    "vllm serve --model mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "            --chat-template examples/tool_chat_template_mistral.jinja \\\n",
    "            --enable-auto-tool-choice --tool-call-parser mistral\n",
    "\n",
    "OR\n",
    "vllm serve --model NousResearch/Hermes-2-Pro-Llama-3-8B \\\n",
    "            --chat-template examples/tool_chat_template_hermes.jinja \\\n",
    "            --enable-auto-tool-choice --tool-call-parser hermes\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"The city to find the weather for, e.g. 'San Francisco'\"\n",
    "                },\n",
    "                \"state\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"the two-letter abbreviation for the state that the city is\"\n",
    "                    \" in, e.g. 'CA' which would mean 'California'\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit to fetch the temperature in\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\", \"state\", \"unit\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hi! How are you doing today?\"\n",
    "}, {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"I'm doing well! How can I help you?\"\n",
    "}, {\n",
    "    \"role\":\n",
    "    \"user\",\n",
    "    \"content\":\n",
    "    \"Can you tell me what the temperate will be in San Jose in degree?\"\n",
    "}]\n",
    "\n",
    "chat_completion = client.chat.completions.create(messages=messages,\n",
    "                                                 model=model,\n",
    "                                                 tools=tools)\n",
    "\n",
    "print(\"Chat completion results:\")\n",
    "print(chat_completion)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "tool_calls_stream = client.chat.completions.create(messages=messages,\n",
    "                                                   model=model,\n",
    "                                                   tools=tools,\n",
    "                                                   stream=True,\n",
    "                                                   tool_choice=\"auto\")\n",
    "\n",
    "chunks = []\n",
    "for chunk in tool_calls_stream:\n",
    "    chunks.append(chunk)\n",
    "    if chunk.choices[0].delta.tool_calls:\n",
    "        print(chunk.choices[0].delta.tool_calls[0])\n",
    "    else:\n",
    "        print(chunk.choices[0].delta)\n",
    "\n",
    "arguments = []\n",
    "tool_call_idx = -1\n",
    "for chunk in chunks:\n",
    "\n",
    "    if chunk.choices[0].delta.tool_calls:\n",
    "        tool_call = chunk.choices[0].delta.tool_calls[0]\n",
    "\n",
    "        if tool_call.index != tool_call_idx:\n",
    "            if tool_call_idx >= 0:\n",
    "                print(\n",
    "                    f\"streamed tool call arguments: {arguments[tool_call_idx]}\"\n",
    "                )\n",
    "            tool_call_idx = chunk.choices[0].delta.tool_calls[0].index\n",
    "            arguments.append(\"\")\n",
    "        if tool_call.id:\n",
    "            print(f\"streamed tool call id: {tool_call.id} \")\n",
    "\n",
    "        if tool_call.function:\n",
    "            if tool_call.function.name:\n",
    "                print(f\"streamed tool call name: {tool_call.function.name}\")\n",
    "\n",
    "            if tool_call.function.arguments:\n",
    "                arguments[tool_call_idx] += tool_call.function.arguments\n",
    "\n",
    "if len(arguments):\n",
    "    print(f\"streamed tool call arguments: {arguments[-1]}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"tool_calls\": chat_completion.choices[0].message.tool_calls\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_current_weather(city: str, state: str, unit: str = \"fahrenheit\") -> str:\n",
    "    try:\n",
    "        location = f\"{city},{state}\"\n",
    "        # wttr.in JSON API\n",
    "        url = f\"https://wttr.in/{location}\"\n",
    "        params = {\n",
    "            \"format\": \"j1\"  # JSON output\n",
    "        }\n",
    "        headers = {\n",
    "            \"User-Agent\": \"curl\"  # ensures plain-text/JSON output :contentReference[oaicite:1]{index=1}\n",
    "        }\n",
    "        resp = requests.get(url, params=params, headers=headers)\n",
    "        data = resp.json()\n",
    "\n",
    "        current = data[\"current_condition\"][0]\n",
    "        temp_c = float(current[\"temp_C\"])\n",
    "        temp_f = float(current[\"temp_F\"])\n",
    "        condition = current[\"weatherDesc\"][0][\"value\"]\n",
    "\n",
    "        if unit.lower().startswith(\"c\"):\n",
    "            return f\"The weather in {city}, {state} is {temp_c:.1f}°C with {condition.lower()}.\"\n",
    "        else:\n",
    "            return f\"The weather in {city}, {state} is {temp_f:.1f}°F with {condition.lower()}.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Failed to fetch weather via wttr.in: {e}\"\n",
    "\n",
    "\n",
    "available_tools = {\"get_current_weather\": get_current_weather}\n",
    "\n",
    "completion_tool_calls = chat_completion.choices[0].message.tool_calls\n",
    "for call in completion_tool_calls:\n",
    "    tool_to_call = available_tools[call.function.name]\n",
    "    args = json.loads(call.function.arguments)\n",
    "    result = tool_to_call(**args)\n",
    "    print(result)\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": result,\n",
    "        \"tool_call_id\": call.id,\n",
    "        \"name\": call.function.name\n",
    "    })\n",
    "\n",
    "chat_completion_2 = client.chat.completions.create(messages=messages,\n",
    "                                                   model=model,\n",
    "                                                   tools=tools,\n",
    "                                                   stream=False)\n",
    "print(\"\\n\\n\")\n",
    "print(chat_completion_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86410343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: get_current_weather\n",
      "Arguments: {\"city\": \"San Francisco\", \"state\": \"CA\", \"unit\": \"fahrenheit\"}\n",
      "Result: The weather in San Francisco, CA is 56.0°F with partly cloudy.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy\")\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_weather(city: str, state: str, unit: str = \"fahrenheit\") -> str:\n",
    "    try:\n",
    "        location = f\"{city},{state}\"\n",
    "        # wttr.in JSON API\n",
    "        url = f\"https://wttr.in/{location}\"\n",
    "        params = {\n",
    "            \"format\": \"j1\"  # JSON output\n",
    "        }\n",
    "        headers = {\n",
    "            \"User-Agent\": \"curl\"  # ensures plain-text/JSON output :contentReference[oaicite:1]{index=1}\n",
    "        }\n",
    "        resp = requests.get(url, params=params, headers=headers)\n",
    "        data = resp.json()\n",
    "\n",
    "        current = data[\"current_condition\"][0]\n",
    "        temp_c = float(current[\"temp_C\"])\n",
    "        temp_f = float(current[\"temp_F\"])\n",
    "        condition = current[\"weatherDesc\"][0][\"value\"]\n",
    "\n",
    "        if unit.lower().startswith(\"c\"):\n",
    "            return f\"The weather in {city}, {state} is {temp_c:.1f}°C with {condition.lower()}.\"\n",
    "        else:\n",
    "            return f\"The weather in {city}, {state} is {temp_f:.1f}°F with {condition.lower()}.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Failed to fetch weather via wttr.in: {e}\"\n",
    "tool_functions = {\"get_weather\": get_weather}\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"The city to find the weather for, e.g. 'San Francisco'\"\n",
    "                },\n",
    "                \"state\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"the two-letter abbreviation for the state that the city is\"\n",
    "                    \" in, e.g. 'CA' which would mean 'California'\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit to fetch the temperature in\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\", \"state\", \"unit\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=client.models.list().data[0].id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "tool_call = response.choices[0].message.tool_calls[0].function\n",
    "print(f\"Function called: {tool_call.name}\")\n",
    "print(f\"Arguments: {tool_call.arguments}\")\n",
    "print(f\"Result: {get_weather(**json.loads(tool_call.arguments))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a23c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
